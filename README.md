# Data_Analysis_with_Airflow_Hadoop_Hbase_For_Mastodon

# Mastodon Data Pipeline

![GitHub](https://img.shields.io/github/license/omardbaa/Data_Analysis_with_Airflow_Hadoop_Hbase_For_MastodoW) ![GitHub derniÃ¨re validation](https://img.shields.io/github/last-commit/omardbaa/Data_Analysis_with_Airflow_Hadoop_Hbase_For_MastodoW) ![GitHub problÃ¨mes](https://img.shields.io/github/issues/omardbaa/Data_Analysis_with_Airflow_Hadoop_Hbase_For_MastodoW)

Data_Analysis_with_Airflow_Hadoop_Hbase_For_Mastodon est un projet de pipeline de donnÃ©es composÃ© de quatre phases principales : extraction de donnÃ©es Ã  partir de l'API Mastodon, traitement des donnÃ©es avec Hadoop MapReduce en utilisant Python en streaming, stockage des donnÃ©es dans HBase et orchestration avec Apache Airflow pour une exÃ©cution automatisÃ©e quotidienne. Le projet vise Ã  collecter et analyser des donnÃ©es de la plateforme Mastodon, une plateforme de mÃ©dias sociaux fÃ©dÃ©rÃ©e.

## AperÃ§u du Projet

### Phase 1: Collecte de DonnÃ©es
- ğŸ“¡ **Extraction de DonnÃ©es :** Utilisez l'API Mastodon avec vos jetons d'accÃ¨s pour collecter des donnÃ©es brutes depuis la plateforme Mastodon.

- ğŸ’¾ **Stockage des DonnÃ©es Brutes :** Stockez les donnÃ©es brutes dans un systÃ¨me de fichiers distribuÃ© tel que HDFS pour garantir la scalabilitÃ©.

- ğŸ—„ï¸ **ModÃ©lisation du Data Lake HDFS :** DÃ©finissez le schÃ©ma du data lake pour HDFS.

### Phase 2: Traitement de DonnÃ©es avec MapReduce
- ğŸ—ºï¸ **Mappeur :** Traitez les donnÃ©es d'entrÃ©e et gÃ©nÃ©rez des paires clÃ©-valeur en fonction des mÃ©triques souhaitÃ©es (par exemple, les abonnÃ©s des utilisateurs, le taux d'engagement, les URL, les emojis, etc.).

- ğŸ“Š **RÃ©ducteur :** AgrÃ©gez les paires clÃ©-valeur produites par le mappeur.

- âš™ï¸ **ExÃ©cution du Travail MapReduce :** Utilisez l'API de streaming Hadoop pour exÃ©cuter la tÃ¢che MapReduce, en fournissant les scripts du mappeur et du rÃ©ducteur comme entrÃ©es.

- ğŸ“ˆ **Surveillance :** Suivez la progression du travail grÃ¢ce Ã  l'interface web Hadoop.

### Phase 3: Stockage des DonnÃ©es dans HBase
- ğŸ—‚ï¸ **Conception du SchÃ©ma HBase :** Concevez le schÃ©ma HBase en fonction des informations que vous souhaitez extraire.

- âš–ï¸ **Meilleures Pratiques :** Suivez les meilleures pratiques pour la conception de la clÃ© de ligne, la conception des familles de colonnes, la compression, les filtres de Bloom, les inserts par lot, etc.

- ğŸ›ï¸ **CrÃ©ation de Tables :** CrÃ©ez les tables nÃ©cessaires dans HBase.

- ğŸ“¥ **Insertion de DonnÃ©es :** Remplissez les rÃ©sultats du rÃ©ducteur dans les tables HBase Ã  l'aide d'un client HBase en Python ou de la mÃ©thode de votre choix.

### Phase 4: Orchestration avec Apache Airflow
- ğŸŒªï¸ **Orchestration du Workflow :** DÃ©finissez un graphe acyclique dirigÃ© (DAG) pour orchestrer l'ensemble du workflow.

- ğŸ“† **CrÃ©ation de TÃ¢ches :** CrÃ©ez des tÃ¢ches pour exÃ©cuter le travail MapReduce et stocker les rÃ©sultats dans HBase.

- ğŸ” **Surveillance et Gestion des Erreurs :** Surveillez la progression et gÃ©rez les erreurs ou les Ã©checs.

### Phase 5: Analyse de DonnÃ©es
AprÃ¨s avoir rÃ©ussi les phases prÃ©cÃ©dentes, vous pouvez effectuer l'analyse de donnÃ©es. Vous Ã©crirez des requÃªtes pour :

#### Analyse des Utilisateurs
- ğŸ§‘â€ğŸ’¼ Identifiez les utilisateurs ayant le plus grand nombre d'abonnÃ©s.

- ğŸ“Š Analysez les taux d'engagement des utilisateurs.

- ğŸ“ˆ Analysez la croissance des utilisateurs au fil du temps en utilisant la mÃ©trique `user_created_at`.

#### Analyse du Contenu
- ğŸ”— Identifiez les sites web externes (URL) les plus partagÃ©s.

#### Analyse de la Langue et de la RÃ©gion
- ğŸŒ Analysez la distribution des publications en fonction de leur langue.

#### Analyse de l'Engagement des MÃ©dias
- ğŸ“· DÃ©terminez le nombre de publications avec des piÃ¨ces jointes multimÃ©dias.

#### Analyse des Balises et des Mentions
- ğŸ·ï¸ Identifiez les balises et les utilisateurs mentionnÃ©s les plus frÃ©quemment utilisÃ©s.

### Phase 6: ExÃ©cution du Workflow
Dans l'interface web Apache Airflow, activez le DAG, surveillez la progression de l'exÃ©cution du DAG et vÃ©rifiez les journaux pour tout problÃ¨me. Une fois le DAG est terminÃ©, examinez les rÃ©sultats dans HBase.

### Phase 7: Optimisation et Surveillance
- âš™ï¸ Optimisez les scripts MapReduce pour de meilleures performances. Surveillez HBase pour tout problÃ¨me de stockage. Configurez des alertes dans Airflow pour les Ã©checs de tÃ¢ches. Surveillez rÃ©guliÃ¨rement Hadoop en utilisant son interface web respective.

### Phase 8: Mise Ã  Jour de la Configuration des Droits d'AccÃ¨s
- ğŸ” Mettez Ã  jour les jetons d'API si les rÃ´les organisationnels changent, en vous assurant qu'ils ont les autorisations nÃ©cessaires pour la rÃ©cupÃ©ration de donnÃ©es.

### Phase 9: Mise Ã  Jour de la Documentation
- ğŸ“„ Mettez Ã  jour la documentation des rÃ¨gles d'accÃ¨s, y compris les dÃ©tails sur les rÃ´les, les autorisations, les demandes d'accÃ¨s et les procÃ©dures d'octroi ou de rÃ©vocation d'accÃ¨s.

### Phase 10: Planification et ConformitÃ©
## ConformitÃ© RGPD dans le Pipeline de DonnÃ©es Mastodon

Pour garantir la conformitÃ© au RGPD, notre pipeline de donnÃ©es suit ces Ã©tapes essentielles :

1. **Anonymisation des DonnÃ©es :**
   - Suppression ou hachage des donnÃ©es personnelles inutiles.

2. **Minimisation des DonnÃ©es :**
   - Conservation uniquement des donnÃ©es nÃ©cessaires Ã  l'analyse.

3. **SÃ©curitÃ© :**
   - SÃ©curisation des systÃ¨mes de stockage HDFS et HBase.

4. **Gestion du Data Lake :**
   - Suppression des donnÃ©es une fois traitÃ©es pour Ã©viter la rÃ©tention excessive.

5. **Ã‰valuation de l'Impact sur la Protection des DonnÃ©es (AIPD) :**
   - Identification et attÃ©nuation des risques pour la vie privÃ©e.

6. **Consentement :**
   - Obtention d'un consentement explicite lorsque requis.

7. **Droits des Personnes ConcernÃ©es :**
   - PossibilitÃ© d'accÃ©der, corriger ou supprimer leurs donnÃ©es.

8. **Documentation et ConformitÃ© :**
   - Tenue de registres dÃ©taillÃ©s pour la transparence.

9. **Audits et VÃ©rifications :**
   - ContrÃ´les rÃ©guliers de conformitÃ© aux rÃ©glementations RGPD.

10. **Gestion des Violations de DonnÃ©es :**
    - Protocole de rÃ©ponse aux violations conforme au RGPD.

Ces mesures dÃ©montrent notre engagement envers la conformitÃ© au RGPD, la protection des donnÃ©es personnelles et le respect des droits Ã  la vie privÃ©e.


## Licence

Ce projet est sous licence MIT. Voir le fichier [LICENSE](LICENSE) pour plus de dÃ©tails.
